# ğŸ•·ï¸ Web Scraper Suite

**L'outil tout-en-un pour rÃ©cupÃ©rer et organiser automatiquement les donnÃ©es de sites web !**

> ğŸ¯ **IdÃ©al pour** : Journalistes, chercheurs, analystes, Ã©tudiants, entrepreneurs, ou toute personne qui veut rÃ©cupÃ©rer facilement des informations sur internet.

## ğŸ¤” Qu'est-ce que c'est ?

Web Scraper Suite est un **outil gratuit et facile** qui vous permet de :
- ğŸ“¥ **RÃ©cupÃ©rer automatiquement** tous les liens d'un site web
- ğŸ“… **Extraire les dates** de publication des articles
- ğŸ“‚ **Organiser** tout par annÃ©e et par mois
- ğŸ“Š **Exporter** vers Excel/CSV pour vos analyses
- ğŸ” **Rechercher** des mots-clÃ©s spÃ©cifiques dans les pages

**Pas besoin d'Ãªtre informaticien !** L'interface est simple avec des boutons et des Ã©tapes claires.

## âœ¨ Pourquoi l'utiliser ?

### ğŸš€ **Gain de temps Ã©norme**
- Au lieu de copier-coller manuellement pendant des heures
- RÃ©cupÃ©rez **des milliers de liens** en quelques minutes
- Organisation automatique par date

### ğŸ¯ **Utilisations concrÃ¨tes**
- **Journalistes** : Analyser l'historique d'un site d'actualitÃ©s
- **Chercheurs** : Constituer un corpus d'articles scientifiques
- **Ã‰tudiants** : Rassembler des sources pour un mÃ©moire
- **Entreprises** : Surveiller la concurrence ou analyser un marchÃ©
- **Blogueurs** : Faire une veille sur un sujet

### ğŸ’¡ **Exemples rÃ©els**
```
ğŸ¯ Analyser 5 ans d'articles du Monde sur l'Ã©cologie
ğŸ¯ RÃ©cupÃ©rer tous les communiquÃ©s de presse d'une entreprise
ğŸ¯ Constituer une base de donnÃ©es d'articles scientifiques
ğŸ¯ Surveiller les mentions d'une marque sur diffÃ©rents sites
```

## ğŸ® Comment Ã§a marche ?

### **Interface simple en 4 Ã©tapes**

```
1ï¸âƒ£ SCRAPER          2ï¸âƒ£ DATES           3ï¸âƒ£ ORGANISATION    4ï¸âƒ£ EXPORT
   ğŸ“¥                   ğŸ“…                  ğŸ“‚                 ğŸ“Š
RÃ©cupÃ©rer          Extraire            Classer            Exporter
les liens          les dates           par annÃ©e          vers Excel
```

### **Exemple concret : Analyser un blog**

1. **Je rentre l'adresse** : `https://blog.exemple.com`
2. **L'outil rÃ©cupÃ¨re** : 2,847 articles automatiquement
3. **Il extrait les dates** : 1,923 articles avec dates trouvÃ©es
4. **Il organise tout** : Par annÃ©es (2018â†’2024) et par mois
5. **J'exporte** : Un beau fichier Excel prÃªt pour l'analyse

â±ï¸ **Temps total** : 15 minutes au lieu de plusieurs semaines !

## ğŸ› ï¸ Installation Super Simple

### **Option 1 : Installation Automatique (RecommandÃ©e)**
1. **ğŸ“¥ Clonez** le projet :
   ```cmd
   git clone https://github.com/maxim-prox-i/web_scraper_project.git
   cd web_scraper_project
   ```
2. **ğŸ–±ï¸ Double-cliquez** sur `install.bat`
3. **â˜• Attendez** que tout s'installe automatiquement
4. **âœ… C'est prÃªt !**

### **Option 2 : Si vous n'avez pas Git**
1. **ğŸ“¥ TÃ©lÃ©chargez** le ZIP depuis https://github.com/maxim-prox-i/web_scraper_project
2. **ğŸ“‚ DÃ©compressez** dans un dossier
3. **ğŸ–±ï¸ Double-cliquez** sur `install.bat`
4. **âœ… C'est prÃªt !**

> ğŸ’¡ **Aucune expertise technique requise** - Les scripts d'installation font tout pour vous !

## ğŸš€ Guide de DÃ©marrage

### **Premier lancement**
1. **Allez** dans le dossier `web_scraper_project`
2. **Double-cliquez** sur `start.bat` (crÃ©Ã© automatiquement)
3. **Une fenÃªtre** s'ouvre avec des onglets simples
4. **Suivez** les Ã©tapes dans l'ordre !

### **Onglet 1 : Scraper de Site** ğŸ•·ï¸
- **URL du site** : Collez l'adresse du site (ex: https://lemonde.fr)
- **Limite** : Combien de pages maximum ? (laissez vide = tout)
- **Cliquez** : "DÃ©marrer le Scraping"

**RÃ©sultat** : Un fichier `urls.txt` avec tous les liens trouvÃ©s

### **Onglet 2 : Extraction de Dates** ğŸ“…
- **Fichier d'entrÃ©e** : SÃ©lectionnez le fichier `urls.txt` crÃ©Ã© avant
- **Nombre de processus** : 10 par dÃ©faut (plus = plus rapide)
- **Cliquez** : "DÃ©marrer l'Extraction"

**RÃ©sultat** : Un fichier CSV avec les liens + leurs dates

### **Onglet 3 : Organisation** ğŸ“‚
- **Fichier CSV** : SÃ©lectionnez le fichier CSV crÃ©Ã© avant
- **Nom du dossier** : Donnez un nom Ã  votre projet
- **Cliquez** : "DÃ©marrer l'Organisation"

**RÃ©sultat** : Dossiers organisÃ©s par annÃ©e et mois

### **Onglet 4 : Export CSV** ğŸ“Š
- **Mode** : Choisissez "Export direct" ou "Depuis structure"
- **Format** : Un fichier Excel ou plusieurs
- **Cliquez** : "DÃ©marrer l'Exportation"

**RÃ©sultat** : Fichier Excel prÃªt pour vos analyses !

## ğŸ“Š RÃ©sultats Typiques

### **Ce que vous obtenez**
```
ğŸ“ Votre Projet/
  ğŸ“ 2024/
    ğŸ“ 01_Janvier/
      ğŸ“„ 2024-01.urls.txt (243 liens)
    ğŸ“ 02_FÃ©vrier/
      ğŸ“„ 2024-02.urls.txt (189 liens)
  ğŸ“ 2023/
    ğŸ“ 12_DÃ©cembre/
      ğŸ“„ 2023-12.urls.txt (156 liens)
  ğŸ“„ export_final.csv (fichier Excel)
```

### **Performance**
- âš¡ **5,000 pages** analysÃ©es en 10 minutes
- ğŸ“… **80% de taux** de rÃ©ussite pour les dates seulement si prÃ©sente dans la page
- ğŸ“Š **Export Excel**

## ğŸ”§ Options AvancÃ©es (Optionnel)

### **ParamÃ¨tres de Performance**
- **Threads** : Plus = plus rapide (attention Ã  ne pas surcharger les sites)
- **DÃ©lais** : Temps d'attente entre les requÃªtes (respect des sites)
- **Limites** : Nombre maximum de pages Ã  traiter

### **Formats de Date SupportÃ©s**
- FranÃ§ais : 15 janvier 2024, 15/01/2024
- Anglais : January 15, 2024, 01/15/2024
- ISO : 2024-01-15
- Dans les URLs : /2024/01/15/

### **Recherche de Mots-clÃ©s**
- Expressions exactes : `"intelligence artificielle"`
- Mots sÃ©parÃ©s : `intelligence, artificielle, IA`
- Respect de la casse : optionnel

## â“ Questions FrÃ©quentes

### **ğŸ¤” C'est lÃ©gal ?**
Oui ! L'outil respecte les rÃ¨gles :
- Lit le fichier `robots.txt` des sites
- Ajoute des dÃ©lais entre les requÃªtes
- Ne surcharge pas les serveurs

### **ğŸ’» Ã‡a marche sur Mac/Linux ?**
L'outil est dÃ©veloppÃ© pour Windows mais fonctionne sur tous les systÃ¨mes avec Python.

### **ğŸ”’ Mes donnÃ©es sont-elles sÃ©curisÃ©es ?**
Tout reste sur votre ordinateur ! Rien n'est envoyÃ© sur internet.

### **âš¡ C'est vraiment gratuit ?**
Oui, complÃ¨tement gratuit et open-source.

### **ğŸ“ J'ai un problÃ¨me, qui peut m'aider ?**
- Lisez le guide INSTALL.md
- Utilisez le script `test_install.py` pour diagnostiquer
- VÃ©rifiez que Python est bien installÃ©

## ğŸ Bonus : Scripts PrÃªts Ã  l'Emploi

### **Mise Ã  jour automatique**
```cmd
update.bat  # Met Ã  jour l'outil avec les derniÃ¨res amÃ©liorations
```

### **Test de fonctionnement**
```cmd
test_install.py  # VÃ©rifie que tout fonctionne correctement
```

### **DÃ©marrage rapide**
```cmd
start.bat  # Lance l'interface directement
```

## ğŸŒŸ Prochaines AmÃ©liorations

- ğŸ¨ Interface encore plus simple
- ğŸ“± Version web (dans le navigateur)
- ğŸ¤– Intelligence artificielle pour analyser le contenu
- ğŸ“ˆ Graphiques automatiques
- ğŸŒ Support multilingue complet

---

## ğŸ’¡ **Vous Ãªtes prÃªt Ã  Ã©conomiser des heures de travail ?**

1. **ğŸ“¥ Clonez** le projet depuis GitHub :
   ```cmd
   git clone https://github.com/maxim-prox-i/web_scraper_project.git
   ```
2. **ğŸ–±ï¸ Double-cliquez** sur `install.bat`  
3. **ğŸš€ Lancez** votre premiÃ¨re analyse avec `start.bat` !

**En 10 minutes, vous serez opÃ©rationnel !**

---

*DÃ©veloppÃ© avec â¤ï¸ pour simplifier la vie de tous ceux qui travaillent avec des donnÃ©es web*